# Big-Data-Analysis

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: NEHA KARAL

*INTERN ID*: CT08DY1991

*DOMAIN*: DATA ANALYST

*DURATION*: 8 WEEKS

*MENTOR*: NEELA SANTOSH

**Project Title: Big Data Analysis using Pyspark on Superstore Sales Dataset**

The object of this project is to perform Big Data Analysis on a large retail dataset to derive meaningful business insights and demonstrate the scalability of big data tools such as **Pyspark**. This project is part of the **CODTECH Internship Task- 1**, which focuses on analyzing large datasets to understand patterns, trends, and performance indicators that can help improve business decision-making processes.

The dataset used for this task, Sample Superstore Cleaned, contains detailed transactional information about sales made by a retail store across different regions. It includes key fields such as **Order ID, Customer Segment, Product Category, Region, Sales, Profit, Quantity, and Discount.** The main aim of the analysis is to explore sales performance, identify profitable and unprofitable areas, and understand customer and product-level trends.

Since the dataset is relatively large, **Pyspark** was chosen as the analysis tool to handle and process data efficiently. Pyspark provides distributed data processing capabilities that make it suitable for handling large-scale data. The data was loaded into a Spark DataFrame, and several **data processing** steps were performed, including handling missing values, verifying data types, and removing duplicates to ensure data quality and consistency.

After cleaning, various analytical operations were performed to extract business insights:
1. **Regional Analysis**: Comparison of total sales and profits across regions to identify high-performing areas.
2. **Category and Sub-Category Analysis**: Evaluation of which product categories contibute most to sales and profitability.
3. **Customer Segmentation**: Analysis of sales performance by customer segment such as Corporate, Consumer, and Home Office.
4. **Discount Impact**: Investigation of the relationship between discount rates and profitable.
5. **Shipping and Order Trends**: Examination of order volumes and delivery modes to identify logistical efficiency.

Additionally, **Pyspark** and **DataFrame APIs** were used to perform aggregations and group-level summaries efficiently. The scalibility of Pyspark allowed smooth processing even when simulating larger datasets, demonstrating its capability for handling big data environments.

Through this project, I developed a strong understanding of big data processing concepts, distributed computing, and data-driven decision-making. The use of Pyspark provided valuable hands-on experience in performing large-scale analytics, aligning with industry practices in data engineering and business intelligence.
